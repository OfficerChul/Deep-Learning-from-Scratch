{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8557488e-d06e-4bc7-866d-f602ba004ca3",
   "metadata": {},
   "source": [
    "# backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984efcc4-2577-4707-9903-a5a4a3928dd7",
   "metadata": {},
   "source": [
    "## MulLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24c0dc92-cfcb-467b-a218-7ab09863f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x*y\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "        \n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c408ab-a498-4d08-a046-4fac33ae05b9",
   "metadata": {},
   "source": [
    "## Buy Apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52c4edde-bc22-491b-8262-5855af648a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "220.00000000000003\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "# Layers\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# Forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "print(apple_price)\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37faaeae-79bd-4aeb-aae8-f6462a195a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple, dapple_num, dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89194130-074b-4029-86e0-b18c057bd420",
   "metadata": {},
   "source": [
    "## AddLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "500ca002-176b-4194-9e82-bace774fcfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5e6728-fac1-4f62-8abd-7fd6289df186",
   "metadata": {},
   "source": [
    "## Buy Apple Orange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "161b2258-fa14-479f-8ab2-840d863ace65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "source": [
    "apple_num = 2\n",
    "apple = 100\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# Layers\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "add_fruit_layer = AddLayer()\n",
    "\n",
    "# Forward\n",
    "apple_price = mul_apple_layer.forward(apple_num, apple)\n",
    "orange_price = mul_orange_layer.forward(orange_num, orange)\n",
    "fruit_price = mul_fruit_layer.forward(apple_price, orange_price)\n",
    "price = mul_tax_layer.forward(fruit_price, tax)\n",
    "\n",
    "dprice = 1\n",
    "# Backward\n",
    "dfruit_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple_price, dorange_price = add_fruit_layer.backward(dfruit_price)\n",
    "dapple_num, dapple = mul_apple_layer.backward(dapple_price)\n",
    "dorange_num, dorange = mul_orange_layer.backward(dorange_price)\n",
    "\n",
    "print(price)\n",
    "print(dapple_num, dapple, dorange, dorange_num, dtax)\n",
    "xt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e780831e-7af3-4190-95fd-1cf2db7059ad",
   "metadata": {},
   "source": [
    "## Activation Function Layer Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927c7c87-88fa-4824-a9ea-2e869e4e14e6",
   "metadata": {},
   "source": [
    "### ReLu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61a9a28d-6d8d-42df-afc2-16878722069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e449bd-b3f6-45eb-b016-49e12cee1885",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3818a28b-734c-4468-8012-634543816265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.out = 1 / (1 + np.exp(-x))\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.out * (1-self.out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8b6133-f758-4bb3-84d6-5db8f7eb7d59",
   "metadata": {},
   "source": [
    "## Affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44aeda02-4727-46af-95fe-b1856dbb1bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self, x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297368d1-796f-480f-bdef-0cdde6fd8708",
   "metadata": {},
   "source": [
    "## Softmax-with-Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e80189cb-4242-4054-bf6e-8d642ca0dff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eb3c4a-ecf1-42ee-8c45-8cb048288ab7",
   "metadata": {},
   "source": [
    "## TwoLayerNet Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c2376c3-aae3-4e53-8b14-9962a31e9b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6466de61-520b-48cf-9f6d-f106a15f3068",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # initialize weight\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(input_size, hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size, output_size)\n",
    "        \n",
    "        # initialize layers\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Reulu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, y_hat):\n",
    "        y = self.predict(x)\n",
    "        loss = self.lastLayer.forward(y, y_hat)\n",
    "        \n",
    "    def accuracy(self, x, y_hat):\n",
    "        y = self.predict(x)\n",
    "        y = argmax(y, axis=1)\n",
    "        if y_hat.ndim != 1:\n",
    "            y_hat = np.argmax(y_hat, axis=1)\n",
    "            \n",
    "        accuracy = np.sum(y == y_hat)/len(y)\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, y_hat):\n",
    "        loss_W = lambda W: self.loss(x. y_hat)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, y_hat):\n",
    "        \n",
    "        # forward\n",
    "        self.loss(x, y_hat)\n",
    "        \n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8833836-a80d-41bc-b569-3ac1f9829d4f",
   "metadata": {},
   "source": [
    "## Gradient Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fc6bfa9-15a7-4c87-91df-9154a7b066b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1: 4.705272852647447e-10\n",
      "b1: 3.1105161162287097e-09\n",
      "W2: 5.765430520393014e-09\n",
      "b2: 1.3952254441923495e-07\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# read data\n",
    "(x_train, y_train), (x_test, y_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "y_batch = y_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, y_batch)\n",
    "grad_backprop = network.gradient(x_batch, y_batch)\n",
    "\n",
    "for layer in grad_numerical.keys():\n",
    "    diff = np.average(abs(grad_backprop[layer]-grad_numerical[layer]))\n",
    "    print(f'{layer}: {str(diff)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44200f0d-2bbc-467f-8e52-734d20117f80",
   "metadata": {},
   "source": [
    "## Training Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96532300-516a-4304-be37-c557ae019d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12288333333333333 0.1245\n",
      "0.9034833333333333 0.9065\n",
      "0.9237666666666666 0.9289\n",
      "0.9362333333333334 0.9376\n",
      "0.9453166666666667 0.9432\n",
      "0.95185 0.9502\n",
      "0.9569333333333333 0.9533\n",
      "0.9600166666666666 0.9568\n",
      "0.96505 0.9596\n",
      "0.9666833333333333 0.9612\n",
      "0.9692833333333334 0.9646\n",
      "0.97195 0.9658\n",
      "0.9740666666666666 0.9672\n",
      "0.97575 0.9687\n",
      "0.9744833333333334 0.9676\n",
      "0.9775333333333334 0.9684\n",
      "0.9786 0.9705\n"
     ]
    }
   ],
   "source": [
    "# read data -> \n",
    "\n",
    "# select some data from the dataset using batch_mask(np.random.choice(train.shape[0], batch_size) ->\n",
    "# train the batched data -> \n",
    "\n",
    "# forward pass: forward pass -> use softmax to get the softed values(normalized values) for each class which is a final prediction ->\n",
    "# get the loss between the final prediction and y_train value\n",
    "\n",
    "# backward pass: differentiate the prediction and the label value to get the loss of the last layer -> \n",
    "# pass the output of the differentiation backward (multiply the derivative to the derivative of the previous layer?)\n",
    "\n",
    "# update the parameters (weights and biases) of the each node by using learning rate\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "    \n",
    "    grad = network.gradient(x_batch, y_batch)\n",
    "    \n",
    "    for param in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[param] -= learning_rate * grad[param]\n",
    "        \n",
    "    loss = network.loss(x_batch, y_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, y_train)\n",
    "        test_acc = network.accuracy(x_test, y_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95e57a4-1b76-4721-913f-4263be08ff29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
